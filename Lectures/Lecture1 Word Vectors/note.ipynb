{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word Vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 表示词汇的方式\n",
    "\n",
    "1. WordNet: 麻烦，不准确\n",
    "2. One-Hot Vector: 无法计算相似度，维度太大\n",
    "3. **Context**: “You shall know a word by the company it keeps”; and **Word Vectors** (word embeddings, or word representations)\n",
    "\n",
    "## Word2Vec\n",
    "\n",
    "![](https://cdn.jsdelivr.net/gh/KinnariyaMamaTanha/Images@main/202408211630429.png)\n",
    "\n",
    "Objective function:\n",
    "\n",
    "$$\n",
    "J(\\theta) = -\\frac{1}{T}\\log L(\\theta) = -\\frac{1}{T}\\sum_{t=1}^{T}\\sum_{-m \\le j \\le m, j\\ne0}\\log P(w_{t+j}|w_t;\\theta) \n",
    "$$\n",
    "\n",
    "- $m$：窗口大小\n",
    "- $t$：center token 位置\n",
    "\n",
    "---\n",
    "\n",
    "每个单词 $w$ 用两个 vectors 表示：\n",
    "\n",
    "- $v_w$：when $w$ is a center word\n",
    "- $u_w$: when context word\n",
    "\n",
    "那么对于 center word $c$ 和 context word $o$\n",
    "\n",
    "$$\n",
    "P(o|c) = \\frac{\\exp (u_o^Tv_c)}{\\sum_{w \\in V}^{}\\exp (u_w^Tv_c)}\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "参数 $\\theta$：当 word vector 是 d 维的，并且总共 $V$ 个单词，有\n",
    "\n",
    "$$\n",
    "\\theta = \\begin{bmatrix}\n",
    "    v_{aardvark} \\\\\n",
    "    v_a \\\\\n",
    "    \\vdots \\\\\n",
    "    v_{zebra} \\\\\n",
    "    u_{aardvark} \\\\\n",
    "    u_a \\\\\n",
    "    \\vdots \\\\\n",
    "    u_{zebra}\n",
    "\\end{bmatrix} \\in \\mathbb{R}^{2dV}\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "Co-Occurrence Matrix:\n",
    "\n",
    "除了 word vector，还可以分别从文件层面/段落层面/句子层面等统计单词共同出现的次数，形成一个矩阵，代表了所有单词之间的关系。不过效果没有 word vector 好。\n",
    "\n",
    "## Objective Function\n",
    "\n",
    "部分推导：\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "    \\frac{\\partial}{\\partial v_c} P(o|c) &= \\frac{\\partial}{\\partial v_c} \\log \\frac{\\exp (u_o^Tv_c)}{\\sum_{w=1}^{V}\\exp (u_w^Tv_c)} \\\\\n",
    "    &= \\frac{\\partial}{\\partial v_c} \\log \\exp (u_o^Tv_c) - \\frac{\\partial}{\\partial v_c} \\log_{} \\sum_{w=1}^{V}\\exp (u_w^Tv_c) \\\\\n",
    "    &=u_o - \\sum_{w=1}^{V}P(w|c)u_w \\\\\n",
    "    &= \\text{observed - expected}\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "## Skip-Gram with Negative Sampling (SGNS)\n",
    "\n",
    "由于在计算损失函数的时候，分母上的 $\\sum_{w=1}^{V}\\exp (u_w^Tv_c)$ 需要计算所有单词和 $c$ 的相似度，速度很慢，所以考虑改进：\n",
    "\n",
    "$$\n",
    "\\log \\sigma (u_o^Tv_c) + \\sum_{l=1}^{k}\\log \\sigma (-u_l^Tv_c)\n",
    "$$\n",
    "\n",
    "这里 $u_l \\sim p_{neg}$，是从未知分布中取样出的 **negative sample**，比如可以在整个单词表中均匀抽样 $k$ 个单词作为 negative sample."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
