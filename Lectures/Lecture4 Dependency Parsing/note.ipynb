{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dependency Parsing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 任务\n",
    "\n",
    "1. **Learning**：给定一系列 sentences 和对应的 dependency graph，学习一个模型 $M$ 来表示对应关系\n",
    "2. **Parsing**：利用得到的 $M$，对给出的新 sentence，计算出对应的 dependency graph\n",
    "\n",
    "## 解决方案\n",
    "\n",
    "### Transition-Based Dependency Parsing\n",
    "\n",
    "基于 state machine，定义出可选的 transitions 来构造出一个从 sentence 到 dependency tree 的过程。\n",
    "\n",
    "- **Learning**：构造一个能预测下一步 transition 的模型\n",
    "- **Parsing**：使用得到的模型，求最优 transitions\n",
    "\n",
    "### Greedy Deterministic Transition-Based Parsing\n",
    "\n",
    "定义 **state** 为三元组 $(\\sigma, \\beta, A)$，其中 $\\sigma$ 为 stack，$\\beta$ 为 buffer，$A$ 为 dependency arcs，由若干表示关系的 $(w_i, r, w_j)$ 组成\n",
    "\n",
    "对任意 sentence $S = w_0w_1\\cdots w_n$，\n",
    "\n",
    "- initial state $c_0 = ([w_0]_\\sigma, [w_1, \\ldots, w_n]_\\beta, \\emptyset)$，只有特殊的 ROOT 在 stack $\\sigma$ 中，其它单词都在 $\\beta$ 中\n",
    "- terminal state $(\\sigma, []_\\beta, A)$\n",
    "\n",
    "定义几种 **transition**：\n",
    "\n",
    "- **Shift**：移除 buffer 中第一个 word，移至 stack 最上面。（前提是 buffer 不空）\n",
    "- **Left Arc_r**：向 dependency arcs 中加入 $(w_j, r, w_i)$，其中 $w_j$ 为 stack 中最上面的，$w_i$ 为第二上面的；然后从 stack 中移除 $w_i$（前提是 stack 中至少两个 word 且 $w_i$ 不是 ROOT）\n",
    "- **Right Arc_r**：向 dependency arcs 中加入 $(w_i, r, w_j)$，其中 $w_j$ 为 stack 中最上面的，$w_i$ 为第二上面的；然后从 stack 中移除 $w_j$（前提是 stack 中至少两个 word）\n",
    "\n",
    "![](https://cdn.jsdelivr.net/gh/KinnariyaMamaTanha/Images@main/202409011819433.png)\n",
    "\n",
    "### Neural Dependency Parsing\n",
    "\n",
    "使用神经网络来训练需要的模型。\n",
    "\n",
    "**Feature Selection**：给定一个 sentence $S$，它的 features 可以定义为\n",
    "\n",
    "- $S_{word}$：对在 stack 和 buffer 的顶部的几个 words (and their dependency) 的向量表示\n",
    "- $S_{tag}$：$S$ 中的几个 word 的 Part of Speech (POS) tags。POS tags 构成一个较小的离散集合 $P$\n",
    "- $S_{label}$：$S$ 中几个 words 的 arc-labels。所有 arc-labels 组成一个较小的离散集合，表示 words 间的关系 $\\mathcal{L} = \\{ amod, tmod, \\ldots\\}$\n",
    "\n",
    "每一种 feature 都有对应的 embedding matrix：\n",
    "\n",
    "- $S_{word} \\rightarrow E^{w} \\in \\mathbb{R}^{d \\times N_w}$\n",
    "- $S_{tag} \\rightarrow E^{t} \\in \\mathbb{R}^{d \\times N_t}$\n",
    "- $S_{label} \\rightarrow E^{l} \\in \\mathbb{R}^{d \\times N_l}$\n",
    "\n",
    "对不存在的 element，使用 NULL 代替。向量化之后就得到了 $E^w, E^t, E^l$ 三个输入。\n",
    "\n",
    "![](https://cdn.jsdelivr.net/gh/KinnariyaMamaTanha/Images@main/202409020807947.png)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cs224n",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
