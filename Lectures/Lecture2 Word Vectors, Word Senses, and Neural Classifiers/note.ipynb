{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lecture 2: Word Vectors, Word Senses, and Neural Network Classifiers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The skip-gram model with negative sampling\n",
    "\n",
    "Objective function:\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "J_t(\\theta) &= \\log \\sigma (u_oTv_c) + \\sum_{i=1}^{k}\\mathbb{E}_{j\\sim P(w)} \\left[\\log \\sigma (-u_j^Tv_c)\\right]\\\\\n",
    "J(\\theta) &= \\frac{1}{T}\\sum_{t=1}^{T}J_t(\\theta)\n",
    "\\end{align*} \n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "Co-ocurrence matrix 问题：\n",
    "\n",
    "1. 直接使用 raw counting matrix 维度过高；但在这个场景下，降维方法例如 SVD 效果不好\n",
    "2. 一些单词出现频率较高但没啥用，例如 \"he\", \"I\", \"her\" 等\n",
    "\n",
    "---\n",
    "\n",
    "改进：\n",
    "\n",
    "1. 在进行 negative sampling 时，希望出现频率更少的单词更有可能被选中为 negative sample，可以选择适当的抽样分布来实现。\n",
    "2. 在计算 co-ocurrence matrix 时，可以考虑多种处理 counting 的方式，例如取对数、限制上界、忽略部分单词、添加距离信息、换用 Pearson correlation 等"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GloVe\n",
    "\n",
    "- 使用 **weighted least squares model**\n",
    "\n",
    "### Co-occurrence Matrix\n",
    "\n",
    "- $X$: co-occurrence matrix\n",
    "- $X_i = \\sum_{k}X_{ik}$: 在单词 $i$ 附近出现的总单词数\n",
    "- $P_{ij} = P(w_j | w_i) = \\frac{X_{ij}}{X_i}$: 单词 $j$ 出现在 $i$ 的 context 中的概率\n",
    "\n",
    "### Least Squares Objective\n",
    "\n",
    "在 skip-gram model 中，单词 $j$ 出现在 $i$ 的 context 中的概率\n",
    "\n",
    "$$\n",
    "Q_{ij} = \\frac{\\exp (u_j^Tv_i)}{\\sum_{w=1}^{W}\\exp (u_w^Tv_i)}\n",
    "$$\n",
    "\n",
    "Global cross-entropy loss:\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "    J&=-\\sum_{i \\in corpus} \\sum_{j \\in context(i)} \\log Q_{ij} \\\\\n",
    "    &= - \\sum_{i=1}^{W}\\sum_{j=1}^{W}X_{ij}\\log Q_{ij}\n",
    "\\end{align*} \n",
    "$$\n",
    "\n",
    "使用这个 loss function 的缺点在计算量太大，$Q$ 需要计算所有单词的余弦相似度并取指数。解决方法：使用 least suqare objective，丢弃掉 $P$ 和 $Q$ 的 normalization factors:\n",
    "\n",
    "$$\n",
    "\\hat{J} = \\sum_{i=1}^{W}\\sum_{j=1}^{W} X_i(\\hat{P}_{ij} - \\hat{Q}_{ij})^2\n",
    "$$\n",
    "\n",
    "这里\n",
    "\n",
    "- $\\hat{P}_{ij} = X_{ij}$\n",
    "- $\\hat{Q}_{ij} = \\exp (u_j^Tv_i)$\n",
    "\n",
    "但这样的话又有一个问题：$X_{ij}$ 通常会很大，不好优化。可以通过取对数来缓解：\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "    \\hat{J} &= \\sum_{i=1}^{W}\\sum_{j=1}^{W} X_i(\\log (\\hat{P}_{ij})_ - \\log (\\hat{Q}_{ij}))^2 \\\\\n",
    "    &= \\sum_{i=1}^{W} \\sum_{j=1}^{W}X_i(u_j^Tv_i - \\log X_{ij})^2\n",
    "\\end{align*} \n",
    "$$\n",
    "\n",
    "又权重 $X_i$ 不一定是最优的权重，可以替换为函数 $f(X_ij)$:\n",
    "\n",
    "$$\n",
    "\\hat{J} = \\sum_{i=1}^{W}\\sum_{j=1}^{W}f(X_{ij})(u_j^Tv_i - \\log X_{ij})^2\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation of Word Vectors\n",
    "\n",
    "产生的 word vectors 也需要被测试质量高低。\n",
    "\n",
    "### Intrinsic Evaluation\n",
    "\n",
    "使用一些中间的子任务（例如 analogy completion 等）来评测，而不是直接使用最终的任务（即 extrinsic evaluation）。\n",
    "\n",
    "例如，如果想搭建一个问答系统，那么可以用 analogy completion 子任务来代替最终任务进行训练：\n",
    "\n",
    "![](https://cdn.jsdelivr.net/gh/KinnariyaMamaTanha/Images@main/202408221313797.png)\n",
    "\n",
    "对于 subtask `a:b :: c:?`，即从条件 `a -> b` 得到 `c` 对应的 vector index `d`：\n",
    "\n",
    "$$\n",
    "d = \\operatorname{argmax}_i \\frac{(x_b - x_a + x_c)^T x_i}{\\left\\| x_b - x_a + x_c \\right\\| }\n",
    "$$"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cs224n",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
